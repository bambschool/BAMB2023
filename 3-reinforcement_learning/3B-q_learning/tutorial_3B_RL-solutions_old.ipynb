{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL tutorial. Part 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will learn how implement different agents that keep track of the value of their actions and the state they are in. We will use 4 different agents:\n",
    "- Random agent.\n",
    "- SARSA agent.\n",
    "- Q-learning agent.\n",
    "- Q-learning agent with eligibility traces."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridWorld"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train our agents to navigate a gridworld. The gridworld is a 2D grid of size 8x8. The agent can move in 4 directions: up, down, left and right, or stay. The agent receives a reward of -1 for each step it takes, a reward of 50 if it reaches the goal state and a reward of -2 if it hits an obstacle. \n",
    "\n",
    "The gridworld will be implemented with a class with the following methods:\n",
    "- `__init__`: initializes the gridworld with a given size, a list of obstacles and a goal state (check the other input parameters).\n",
    "- `new_trial`: resets the state of the agent to the start position and the count of steps within the trial.\n",
    "- `step`: takes an action as input and returns the next state, the reward and a dictionary with a boolean indicating if the trial is finished.\n",
    "- `render`: prints the gridworld with the current position of the agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1688486654982,
     "user": {
      "displayName": "Manuel Molano",
      "userId": "02614548693511315823"
     },
     "user_tz": -120
    },
    "id": "6tUcGXS2Z-g0"
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "# from copy import deepcopy\n",
    "\n",
    "\n",
    "# We now create a class that will contain the task. \n",
    "# This class will be called to generate the trials that we will use to train the agent.\n",
    "class GridWorld():\n",
    "    \"\"\"\n",
    "     GridWorld task.\n",
    "\n",
    "    Args:\n",
    "        size: int, size of the grid\n",
    "        start: tuple, starting position of the agent\n",
    "        goal: tuple, goal position of the agent\n",
    "        obstacles: list of tuples, positions of the obstacles\n",
    "    \"\"\"\n",
    "    def __init__(self, size=5, start=(0, 0), goal=(4, 4), obstacles=None,\n",
    "                 max_steps_per_trial=50):\n",
    "        # define the task parameters\n",
    "        self.size = size\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.obstacles = obstacles or []\n",
    "        self.max_steps_per_trial = max_steps_per_trial\n",
    "        # observation space\n",
    "        self.observation_space = spaces.Box(-np.inf, np.inf,\n",
    "                                            shape=(9,), dtype=np.float32)\n",
    "        # action space\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "        # rewards for reaching the goal (correct), not reaching the goal (fail) and hitting an obstacle\n",
    "        self.rewards = {'correct': 50, 'obstacle': -2, 'fail': -1}\n",
    "        self.position = self.start\n",
    "        # step within the trial\n",
    "        self.t = 0\n",
    "        self.trial = 0\n",
    "\n",
    "    def new_trial(self, **kwargs):\n",
    "        \"\"\"\n",
    "        new_trial() is called when a trial ends to generate the next trial.\n",
    "        kwargs could be used to modify the goal or start positions.    \n",
    "        \"\"\"\n",
    "        self.t = 0\n",
    "        self.trial += 1\n",
    "        self.position = self.start\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        step receives an action and returns:\n",
    "            a new observation, the position of the agent\n",
    "            reward associated with the action, reward\n",
    "            a dictionary with extra information:\n",
    "                boolean indicating the end of the trial, info['new_trial']\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "        new_trial = False\n",
    "        candidate = self.position\n",
    "        # process action\n",
    "        if action == 1:  # Move up\n",
    "            candidate = (max(self.position[0] - 1, 0), self.position[1])\n",
    "        elif action == 2:  # Move right\n",
    "            candidate = (self.position[0], min(self.position[1] + 1, self.size - 1))\n",
    "        elif action == 3:  # Move down\n",
    "            candidate = (min(self.position[0] + 1, self.size - 1), self.position[1])\n",
    "        elif action == 4:  # Move left\n",
    "            candidate = (self.position[0], max(self.position[1] - 1, 0))\n",
    "        \n",
    "        # Updtate reward and position\n",
    "        if candidate == self.goal:\n",
    "            reward = self.rewards['correct']\n",
    "            self.position = self.start\n",
    "            new_trial = True\n",
    "            self.new_trial()\n",
    "        elif candidate in self.obstacles:\n",
    "            reward = self.rewards['obstacle']\n",
    "        else:\n",
    "            self.position = candidate\n",
    "            reward = self.rewards['fail']\n",
    "\n",
    "        # check time limit\n",
    "        if self.t > self.max_steps_per_trial:\n",
    "            new_trial = True\n",
    "            self.new_trial()\n",
    "\n",
    "        return self.position, reward, {'new_trial': new_trial}\n",
    "\n",
    "    def render(self, ax=None):\n",
    "        \"\"\"\n",
    "        Display position of the agent, obstacles and goal in a grid.\n",
    "        \"\"\"\n",
    "        grid = np.zeros((self.size, self.size))\n",
    "        grid[self.position] = 1  # agent's position marked with 1\n",
    "        grid[self.goal] = 2  # goal position marked with 0.5\n",
    "        for obstacle in self.obstacles:\n",
    "            grid[obstacle] = -1\n",
    "        # pad grid with -1\n",
    "        grid = np.pad(grid, pad_width=1, mode='constant', constant_values=-1)\n",
    "        if ax is None:\n",
    "            _, ax = plt.subplots()\n",
    "        ax.imshow(grid, vmin=-1, vmax=2, cmap='hot')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define gridworld parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 5\n",
    "start = (0, 0)\n",
    "goal = (grid_size - 1, grid_size - 1)\n",
    "obstacles = [(x, grid_size//2) for x in range(grid_size-2)]\n",
    "max_steps_per_trial = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot an example environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = GridWorld(size=grid_size, start=start, goal=goal, obstacles=obstacles, max_steps_per_trial=max_steps_per_trial)\n",
    "actions = ['stay', 'up', 'right', 'down', 'left']\n",
    "good_path = [3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 2]\n",
    "f, ax = plt.subplots(figsize=(10, 10), nrows=3, ncols=5)\n",
    "ax = ax.flatten()\n",
    "for i_a, action in enumerate(good_path):\n",
    "    env.step(action)\n",
    "    env.render(ax=ax[i_a])\n",
    "    # remove xtiks and yticks\n",
    "    ax[i_a].set_xticks([])\n",
    "    ax[i_a].set_yticks([])\n",
    "    # add title\n",
    "    ax[i_a].set_title(actions[action])\n",
    "# remove last axis\n",
    "ax[-1].axis('off')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward_per_episode(reward_per_episode, ax=None):   \n",
    "    # Plot rewards\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(6, 6))\n",
    "        ax.set_title(\"Reward per episode\")\n",
    "        ax.set_xlabel(\"Episode\")\n",
    "        ax.set_ylabel(\"Total Reward\")\n",
    "    ax.plot(reward_per_episode)\n",
    "    # plt.show()\n",
    "\n",
    "def plot_q_table(agent, action_space):\n",
    "    # plot q_table\n",
    "    f, ax = plt.subplots(ncols=2, nrows=3, figsize=(10, 10))\n",
    "    ax = ax.flatten()\n",
    "    # actions\n",
    "    titles = ['Stay', 'Up', 'Right', 'Down', 'Left']\n",
    "    for i_a, a in enumerate(ax):\n",
    "        if i_a < len(action_space):\n",
    "            im = a.imshow(agent.get_q_table()[:, :, i_a], cmap='hot')\n",
    "            plt.colorbar(im, ax=a)\n",
    "            a.set_title(titles[i_a])\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function we will use to train all our agents. Think of what you need from the agent to update the gridworld and what the agent needs from the gridworld to act and learn to make better decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "executionInfo": {
     "elapsed": 653,
     "status": "ok",
     "timestamp": 1688486658377,
     "user": {
      "displayName": "Manuel Molano",
      "userId": "02614548693511315823"
     },
     "user_tz": -120
    },
    "id": "mQYTLow4Z-g3",
    "outputId": "d09e1b8a-1783-46c7-82d5-98a3a86acc74"
   },
   "outputs": [],
   "source": [
    "def train_agent(agent, grid_size=5, start=(0, 0), goal=(4, 4), obstacles=[(x, 3) for x in range(3)],\n",
    "                max_num_tr=100, verbose=False, max_steps_per_trial=50):\n",
    "    \"\"\"\n",
    "    Create gridWorld and agent, and train agent for a number of trials.\n",
    "\n",
    "    Args:\n",
    "        agent: agent to train\n",
    "        grid_size: int, size of the grid\n",
    "        start: tuple, starting position of the agent\n",
    "        goal: tuple, goal position for the agent\n",
    "        obstacles: list of tuples, positions of the obstacles\n",
    "        max_num_tr: int, maximum number of trials\n",
    "        verbose: bool, whether to display the environment\n",
    "        max_steps_per_trial: int, maximum number of steps per trial\n",
    "    \"\"\"\n",
    "    # Create grid world\n",
    "    env = GridWorld(size=grid_size, start=start, goal=goal, obstacles=obstacles, max_steps_per_trial=max_steps_per_trial)\n",
    "\n",
    "    # INSTRUCTION 1: first action of the agent. Which parameters do you need to pass to the agent?\n",
    "    action = agent.get_action(state=start)\n",
    "    state, _, _ = env.step(action)\n",
    "    reward_per_episode = []\n",
    "    total_reward = 0\n",
    "    while env.trial < max_num_tr:\n",
    "        # INSTRUCTION 2: get the next action from the agent.\n",
    "        action = agent.get_action(state=state)\n",
    "        next_state, reward, info = env.step(action)\n",
    "        # INSTRUCTION 3: learn from the experience. Which parameters do you need to pass to the agent?\n",
    "        agent.learn(state=state, action=action, reward=reward, next_state=next_state)\n",
    "        state = next_state\n",
    "        if verbose:\n",
    "            print(f\"Trial: {env.trial + 1}, Step: {env.t + 1}, State: {state}, Action: {action}, Next State: {next_state},\\\n",
    "                    Reward: {reward}, Total Reward: {total_reward}, New trial: {info['new_trial']}\")\n",
    "        total_reward += reward\n",
    "        if info['new_trial']: # check if the trial has ended\n",
    "            reward_per_episode.append(total_reward)\n",
    "            total_reward = 0\n",
    "    return reward_per_episode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random agent does not care about the environment and choses an action from the available ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random: \n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "    # INSTRUCTION 4: return the agent's action.\n",
    "    # (hint: you might need to add some dummy input parameters (i.e. not used) so the function works well with the train_agent function)\n",
    "    def get_action(self, **kwargs):  # TODO: explain kwargs\n",
    "        return np.random.choice(self.action_space)\n",
    "    # INSTRUCTION 5: learn from experience (if needed)\n",
    "    # (hint: you might need to add some dummy input parameters (i.e. not used) so the function works well with the train_agent function)\n",
    "    def learn(self, **kwargs): \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [0, 1, 2, 3, 4]  # up, right, down, left\n",
    "max_num_tr = 1000\n",
    "verbose = True\n",
    "agent = Random(action_space=action_space)\n",
    "reward_per_episode_random =\\\n",
    "      train_agent(agent=agent, grid_size=grid_size, start=start, goal=goal,\n",
    "                  obstacles=obstacles, max_num_tr=max_num_tr,\n",
    "                  verbose=verbose, max_steps_per_trial=max_steps_per_trial)\n",
    "plot_reward_per_episode(reward_per_episode_random)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see there random agent does not do great, but its performance will be a good baseline for us to evaluate the learning of the rest of the agents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state-action-reward-state-action (SARSA) agent uses an on-line policy to learn the best policy. The SARSA algorithm works by maintaining a table of action-value estimates Q(s, a) (the q_table), where s is the state and a is the action taken by the agent in that state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent:\n",
    "    def __init__(self, action_space, state_space, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = np.random.rand(self.state_space[0], self.state_space[1], len(self.action_space))\n",
    "    # INSTRUCTION 6: return the agent's action.\n",
    "    def get_action(self, state):\n",
    "        # INSTRUCTION 6.1: implement epsilon-greedy policy\n",
    "        if state is None or np.random.rand() < self.epsilon:  # Explore: select a random action\n",
    "            return np.random.choice(self.action_space)\n",
    "        else:                                # Exploit: select the action with max value (greedy policy)\n",
    "            return np.argmax(self.q_table[state[0], state[1]])\n",
    "    # INSTRUCTION 7: learn from experience\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        # INSTRUCTION 7.1: get old value from the q-table\n",
    "        old_value = self.q_table[state[0], state[1], action]\n",
    "        # INSTRUCTION 7.2: get action from the q-table\n",
    "        next_action = self.get_action(next_state)\n",
    "        # INSTRUCTION 7.3: get the maximum value from the q-table for the next state and action\n",
    "        next_max = self.q_table[next_state[0], next_state[1], next_action]\n",
    "        # INSTRUCTION 7.4: update the q-table\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n",
    "        self.q_table[state[0], state[1], action] = new_value\n",
    "    def get_q_table(self):\n",
    "        return self.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = (grid_size, grid_size)\n",
    "epsilon = 0.1\n",
    "agent = SARSAAgent(action_space=action_space, state_space=state_space, epsilon=epsilon)\n",
    "reward_per_episode_sarsa =\\\n",
    "      train_agent(agent=agent, grid_size=grid_size, start=start, goal=goal,\n",
    "                  obstacles=obstacles, max_num_tr=max_num_tr,\n",
    "                  verbose=verbose, max_steps_per_trial=max_steps_per_trial)\n",
    "plot_reward_per_episode(reward_per_episode_sarsa)\n",
    "plot_q_table(agent, action_space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-learning agent uses an off-line policy to learn about the environment. As the SARSA algorithm, it works by maintaining a table of action-value estimates Q(s, a). The difference between the two is in the way they update the q-table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1688486663835,
     "user": {
      "displayName": "Manuel Molano",
      "userId": "02614548693511315823"
     },
     "user_tz": -120
    },
    "id": "uxAlrPLZ36Bn"
   },
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, action_space, state_space, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = np.random.rand(self.state_space[0], self.state_space[1], len(self.action_space))\n",
    "    # INSTRUCTION 8: return the agent's action.\n",
    "    def get_action(self, state):\n",
    "        # INSTRUCTION 8.1: implement epsilon-greedy policy\n",
    "        if state is None or np.random.rand() < self.epsilon:  # Explore: select a random action\n",
    "            return np.random.choice(self.action_space)\n",
    "        else:                                # Exploit: select the action with max value (greedy policy)\n",
    "            return np.argmax(self.q_table[state[0], state[1]])\n",
    "    # INSTRUCTION 9: learn from experience\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        # INSTRUCTION 9.1: get old value from the q-table\n",
    "        old_value = self.q_table[state[0], state[1], action]\n",
    "        # INSTRUCTION 9.2: get the maximum value from the q-table for the next state and action. This is different from SARSA!\n",
    "        next_max = np.max(self.q_table[next_state[0], next_state[1]])\n",
    "        # INSTRUCTION 9.3: update the q-table\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n",
    "        self.q_table[state[0], state[1], action] = new_value\n",
    "    def get_q_table(self):\n",
    "        return self.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = (grid_size, grid_size)\n",
    "epsilon = 0.1\n",
    "agent = QLearningAgent(action_space=action_space, state_space=state_space, epsilon=epsilon)\n",
    "\n",
    "reward_per_episode_ql =\\\n",
    "      train_agent(agent=agent, grid_size=grid_size, start=start, goal=goal,\n",
    "                  obstacles=obstacles, max_num_tr=max_num_tr,\n",
    "                  verbose=verbose, max_steps_per_trial=max_steps_per_trial)\n",
    "plot_reward_per_episode(reward_per_episode_ql)\n",
    "plot_q_table(agent, action_space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Agent with elegibility trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1688486846817,
     "user": {
      "displayName": "Manuel Molano",
      "userId": "02614548693511315823"
     },
     "user_tz": -120
    },
    "id": "0MO7qwlW3_7s"
   },
   "outputs": [],
   "source": [
    "class QLearningAgent_with_eligibility:\n",
    "    def __init__(self, action_space, state_space, alpha=0.1, gamma=0.9, epsilon=0.1, lambda_=0.9):\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lambda_ = lambda_\n",
    "        self.q_table = np.random.rand(self.state_space[0], self.state_space[1], len(self.action_space))\n",
    "        self.eligibility_trace = np.zeros((self.state_space[0], self.state_space[1]))\n",
    "    # INSTRUCTION 10: return the agent's action.\n",
    "    def get_action(self, state):\n",
    "        if state is None or np.random.rand() < self.epsilon:  # Explore: select a random action\n",
    "            return np.random.choice(self.action_space)\n",
    "        else:  # Exploit: select the action with max value (greedy policy)\n",
    "            return np.argmax(self.q_table[state[0], state[1]])\n",
    "    # INSTRUCTION 11: learn from experience\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        # INSTRUCTION 11.1: get old value from the q-table\n",
    "        old_value = self.q_table[state[0], state[1], action]\n",
    "        # INSTRUCTION 11.2: update eligibility trace\n",
    "        self.eligibility_trace *= self.gamma * self.lambda_\n",
    "        # INSTRUCTION 11.3: update eligibility trace for the current state and action\n",
    "        self.eligibility_trace[state[0], state[1]] += 1\n",
    "        # INSTRUCTION 11.4: get the maximum value from the q-table for the next state and action        \n",
    "        next_max = np.max(self.q_table[next_state[0], next_state[1]])\n",
    "\n",
    "        # INSTRUCTION 11.5: calculate the temporal difference delta\n",
    "        delta = reward + self.gamma * next_max - old_value\n",
    "\n",
    "        # INSRTUCTION 11.6: Update Q-value using eligibility trace\n",
    "        self.q_table[state[0], state[1], action] += self.alpha * delta * self.eligibility_trace[state[0], state[1]]\n",
    "    def get_q_table(self):\n",
    "        return self.q_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGaggLJm1hG7"
   },
   "outputs": [],
   "source": [
    "state_space = (grid_size, grid_size)\n",
    "epsilon = 0.1\n",
    "alpha = 0.1\n",
    "gamma = 0.9 \n",
    "lambda_= 0.9\n",
    "agent = QLearningAgent_with_eligibility(action_space=action_space, state_space=state_space, epsilon=epsilon,\n",
    "                                        alpha=alpha, gamma=gamma, lambda_=lambda_)\n",
    "reward_per_episode_et =\\\n",
    "      train_agent(agent=agent, grid_size=grid_size, start=start, goal=goal,\n",
    "                  obstacles=obstacles, max_num_tr=max_num_tr,\n",
    "                  verbose=verbose, max_steps_per_trial=max_steps_per_trial)\n",
    "plot_reward_per_episode(reward_per_episode_et)\n",
    "plot_q_table(agent, action_space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "f, ax = plt.subplots(figsize=(6, 6))\n",
    "plot_reward_per_episode(reward_per_episode_random, ax=ax)\n",
    "plot_reward_per_episode(reward_per_episode_sarsa, ax=ax)\n",
    "plot_reward_per_episode(reward_per_episode_ql, ax=ax)\n",
    "plot_reward_per_episode(reward_per_episode_et, ax=ax)\n",
    "# add legend to ax\n",
    "ax.legend(['Random', 'SARSA', 'Q-Learning', 'Q-Learning with Eligibility Trace'])\n",
    "ax.set_title(\"Reward per episode\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Total Reward\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1DJApFI4Ej4ObhnD46r4GiwfHEu7-Icc_",
     "timestamp": 1688399249608
    }
   ]
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
