{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HR0Ss_uQRgmu"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neurogym/ngym_usage/blob/master/supervised/auto_notebooks/supervised/PerceptualDecisionMaking-v0.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nkSBo1a7Rgmy"
   },
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will learn how to use recurrent neural networks (RNNs) to solve a Perceptual Decision Making task. Here is the index of the tutorial:\n",
    "\n",
    "- Installs, packages, auxiliary functions.\n",
    "- Preparing for the training:\n",
    "    - Training parameters.\n",
    "    - Define the task (sample dataset).\n",
    "    - Define the network.\n",
    "    - Define the algorithm to train the network.\n",
    "    - Save config.\n",
    "- Supervised training of the RNN.\n",
    "- Run the trained network (and save the behavioral data).\n",
    "- Network analysis:\n",
    "    - Behavioral analysis.\n",
    "    - General neural analysis.\n",
    "    - Stimulus and choice decoding from network activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installs, packages, auxiliary functions\n",
    "\n",
    "### 0.1. Helper functions\n",
    "\n",
    "We first download the helper modules from the [BAMB2024 Github repository](https://github.com/bambschool/BAMB2024). Just execute the code in the cells below and see if it populates the modules locally and ensure that there are no installation errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the github path and authorization token \n",
    "RAW_GITHUB_PATH = 'https://raw.githubusercontent.com/bambschool/BAMB2023/main/4-recurrent_neural_networks/'\n",
    "\n",
    "# define the list of files to be downloaded\n",
    "files = ['data.py', 'ops.py']\n",
    "\n",
    "for file in files:\n",
    "    !wget -O {file} \"{RAW_GITHUB_PATH}{file}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Neurogym\n",
    "\n",
    "We will use the toolbox [**Neurogym**](https://github.com/neurogym/neurogym/) to define the tasks that our RNNs will have to learn. \n",
    "\n",
    "**NeuroGym** is a curated collection of neuroscience tasks with a common interface. It's main goal is to facilitate training of neural network models on neuroscience tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uncomment following lines to install\n",
    "! pip install gym   # Install gym\n",
    "! git clone https://github.com/gyyang/neurogym.git  # Install neurogym\n",
    "%cd neurogym/\n",
    "! pip install -e ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1Suk5bvKiFqZCmIu4f1pWNghBxrznJ0U_\" alt=\"drawing\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XCv_jh_0Rgmz"
   },
   "source": [
    "### 0.3. Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the packages we need for training and analyzing the RNNs. Especially, we will use the toolbox [pytorch](https://pytorch.org/) to define and train the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Kof72wUxRgm0"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpl\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# import torch and neural network modules to build RNNs\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# import gym and neurogym to create tasks\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# packages to save data\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# packages to handle data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# packages to visualize data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# import torch and neural network modules to build RNNs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# import gym and neurogym to create tasks\n",
    "import gym\n",
    "import neurogym as ngym\n",
    "from neurogym.utils import plotting\n",
    "\n",
    "# import\n",
    "import data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing for the training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first set some parameters for the training.\n",
    "\n",
    "Then we will focus on the 3 important decisions one has to make before training the network:\n",
    "- Define the task.\n",
    "- Define the network.\n",
    "- Define the algorithm to train the network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Define the training parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define some training parameters:\n",
    "- *dt*: time step of the task. The task is defined in discrete time steps of 100 ms.\n",
    "- *lr*: learning rate of the network.\n",
    "- *n_epochs*: number of epochs to train the network.\n",
    "- *seq_len*: number of time steps in each trial. The network will be trained on sequences of 100 time steps.\n",
    "- *batch_size*: number of trials per batch. During training, the network will be trained on batches of trials that contain the inputs to the network and the target outputs (or labels).\n",
    "\n",
    "Some of these parameters might be more clear later, when we define the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# name of the task on the neurogym library\n",
    "envid = 'PerceptualDecisionMaking-v0'\n",
    "# INSTRUCTION 19 (once you have finished the tutorial): Other tasks to try: 'DelayMatchSample-v0', 'GoNogo-v0', 'ContextDecisionMaking-v0'\n",
    "# Set up config:\n",
    "training_kwargs = {'dt': 100,\n",
    "                   'lr': 1e-2,\n",
    "                   'n_epochs': 2000,\n",
    "                   'batch_size': 16,\n",
    "                   'seq_len': 100,\n",
    "                   'envid': envid}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Define task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the task parameters:\n",
    "- *dt*: already defined above.\n",
    "- *timing*: timing of the different periods of the task: fixation, stimulus, delay, decision.\n",
    "- *sigma*: standard deviation of the Gaussian noise that will be added to the stimulus.\n",
    "- *dim_ring*: number of stimuli. Our task will have 2 stimuli and two corresponding choices, i.e. it will be a two-alternative forced choice (2AFC) task. However, the task can be generalized to any number of stimuli and choices.\n",
    "\n",
    "Once you have your network trained, you can try to change some of these parameters to see how the network performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Set up task parameters\n",
    "if envid == 'PerceptualDecisionMaking-v0':\n",
    "    env_kwargs = {'dt': training_kwargs['dt'],\n",
    "                'timing': {'fixation': 300, \n",
    "                            'stimulus': 1000, \n",
    "                            'delay': 0, \n",
    "                            'decision': 300}, \n",
    "                'sigma': 2,                \n",
    "                'dim_ring': 2} \n",
    "    wrappers_kwargs = {'probs': [0.8, 0.2]}              \n",
    "else:\n",
    "    env_kwargs = {'dt': training_kwargs['dt']}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Sample dataset\n",
    "\n",
    "After having defined how the task (*the environment*) is implemented and which are the training hyperparameters, we now define a function that builds the environment. This function also provides a dataset object, from which we will sample the trials and labels (targets) that will be used to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def get_dataset(envid, env_kwargs, training_kwargs):\n",
    "    \"\"\"\n",
    "    Create neurogym dataset and environment. \n",
    "\n",
    "    args:\n",
    "        envid (str): name of the task on the neurogym library\n",
    "        env_kwargs (dict): task parameters\n",
    "        training_kwargs (dict): training parameters\n",
    "    \n",
    "    returns:\n",
    "        dataset (neurogym.Dataset): dataset object from which we can sample trials and labels\n",
    "        env (gym.Env): task environment\n",
    "    \"\"\"\n",
    "\n",
    "    # Make supervised dataset using neurogym's Dataset class\n",
    "    dataset = data.Dataset(envid, \n",
    "                           env_kwargs=env_kwargs, wrappers_kwargs=wrappers_kwargs,\n",
    "                           batch_size=training_kwargs['batch_size'],\n",
    "                           seq_len=training_kwargs['seq_len'])\n",
    "    env = dataset.env\n",
    "    \n",
    "    return dataset, env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# call function to sample\n",
    "dataset, env = get_dataset(envid=envid, env_kwargs=env_kwargs, wrappers_kwargs=wrappers_kwargs, training_kwargs=training_kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *dataset* object provides the inputs and targets to train the RNN. The inputs are the stimuli that the network will receive at each time step. The targets are the labels that the network will have to predict at each time step. In our case, the labels are the choices that the network will have to make at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "inputs, labels = dataset()\n",
    "print('inputs shape:', inputs.shape)\n",
    "print('labels shape:', labels.shape)\n",
    "print('Example inputs:')\n",
    "print('Fixation     Stimulus Left Stimulus Right')\n",
    "print(inputs[:20, 0, :])\n",
    "print('Example labels:')\n",
    "print(labels[:20, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, we will train our RNNs with inputs of shape 100 x 16 x 3, corresponding to the lenght of the samples, the batch size and the number of inputs (stimuli). The targets will have shape 100 x 16, corresponding to the lenght of the samples and the batch size.\n",
    "\n",
    "Let's now plot a few trials for an random agent performing the task. Feel free to run it several times, or explore different agents (random, constant) to get to understand the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# INSTRUCTION 0. Play with the task (run it several times, try random and constant agents) to see how it works.\n",
    "mpl.rcParams['font.family'] = ['DejaVu Serif']\n",
    "num_steps = 40\n",
    "inputs = []\n",
    "actions = []\n",
    "gt = []\n",
    "perf = []\n",
    "trial_count = 0\n",
    "for stp in range(int(num_steps)):\n",
    "    action = env.action_space.sample()\n",
    "    # Yoy can also try to set the action to one constant value, e.g. action = 1 \n",
    "    ob, rew, done, info = env.step(action)\n",
    "    inputs.append(ob)\n",
    "    actions.append(action)\n",
    "    gt.append(info['gt'])\n",
    "    if info['new_trial']:\n",
    "        perf.append(info['performance'])\n",
    "    else:\n",
    "        perf.append(0)\n",
    "\n",
    "\n",
    "data = {'ob': np.array(inputs).astype(float),\n",
    "        'actions': actions, 'gt': gt}\n",
    "# Plot\n",
    "f, ax = plt.subplots(ncols=1, nrows=3, figsize=(8, 4), dpi=150, sharex=True)\n",
    "\n",
    "ax[0].plot(np.arange(1, num_steps+1)*env_kwargs['dt'], data['ob'][:, 0], label='Fixation')\n",
    "ax[0].plot(np.arange(1, num_steps+1)*env_kwargs['dt'], data['ob'][:, 1], label='Stim. L.')\n",
    "ax[0].plot(np.arange(1, num_steps+1)*env_kwargs['dt'], data['ob'][:, 2], label='Stim. R.')\n",
    "ax[0].set_ylabel('Inputs')\n",
    "ax[0].legend()\n",
    "ax[1].plot(np.arange(1, num_steps+1)*env_kwargs['dt'], data['gt'], label='Targets', color='k')\n",
    "ax[1].plot(np.arange(1, num_steps+1)*env_kwargs['dt'], data['actions'], label='Choice', linestyle='--')\n",
    "ax[1].set_ylabel('Actions / Targets')\n",
    "ax[1].legend()\n",
    "ax[2].plot(np.arange(1, num_steps+1)*env_kwargs['dt'], perf, label='perf')\n",
    "ax[2].set_ylabel('Performance')\n",
    "ax[2].set_xlabel('Time (ms)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the inputs (observations) have size 3: two stimuli for the left/right choices (green and orange traces) and one fixation cue (blue trace) that indicates when the network should choose. The goal of the RNN will be to identify which of the two stimuli is larger and to choose the corresponding action.\n",
    "\n",
    "In the second panel, the target output (ground-truth) is indicated by the dashed green line and the actions in blue come from a random agent. The bottom panel show the performance of the agent (accuracy) at the end of each trial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Define network\n",
    "\n",
    "We will now define the network that we want to train. As you have seen in the lecture, RNNs are defined by a set of units that are connected to each other. \n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ENHA_qKp3Cl_D0qagUMAJTLHKdJpA1nb\" alt=\"drawing\" width=\"1000\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "RNNs can receive an input (x) that changes from timestep to timestep and produce an associated output (y). The activity of their units at any timestep ($h_t$) is passed to the network as an extra input in the next timestep ($h_{t+1}$). \n",
    "\n",
    "The connections between the different units will be defined by a matrix of weights ($W_R$) that is initialized with random values.\n",
    "\n",
    "Then, we need to define the transfer function of the units ($g_h$). We will use rectified linear units which just take the positive part of the input.\n",
    "\n",
    "Finally, we will readout the units activity with a linear projection to the output units ($g_y$).\n",
    "\n",
    "Let's first define the parameters of the network:\n",
    "- the number (*dimensionality*) of inputs.\n",
    "- the number of hidden units (neurons) in the network.\n",
    "- the dimensionality of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Set up config:\n",
    "num_neurons = 64\n",
    "\n",
    "net_kwargs = {'hidden_size': num_neurons,\n",
    "              'action_size': env.action_space.n,\n",
    "              'input_size': env.observation_space.shape[0]} # size of the input to the network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we define a `Net` class, which creates our network from standard modules in `pytorch`. As you know from previous tutorials, classes can have associated methods, which are functions specified uniquely for object of the respective class. Our Net class will have:\n",
    "- An `_init_` method that will be run when a new object is created and defines the network architecture.\n",
    "- A `forward` method, that receives the input and returns the output of the network (taking into account the network's architecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # INSTRUCTION 1: build a recurrent neural network with a single recurrent layer and rectified linear units\n",
    "        self.vanilla = nn.RNN(input_size, hidden_size, nonlinearity='relu')\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # INSTRUCTION 2: get the output of the network for a given input\n",
    "        out, _ = self.vanilla(x)\n",
    "        x = self.linear(out)\n",
    "        return x, out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create an *instance* of the `Net` class with the desired hyperparameters as defined above. We will then pass this network to our GPU for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Define network instance from the Net class\n",
    "net = Net(input_size=env.observation_space.shape[0],\n",
    "          hidden_size=net_kwargs['hidden_size'],\n",
    "          output_size=env.action_space.n)\n",
    "\n",
    "# Move network to the device (CPU or GPU)\n",
    "net = net.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Define the algorithm to train the network\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To teach the network to perform the task, we will use a supervised learning approach. This means that we will provide the network with the inputs and the target outputs (or labels) and we will train the network to predict the target outputs from the inputs. For that we need a loss function that quantifies the difference between the network's predictions and the target outputs. Here's a screenshot of this morning presentation for you to remember how we train a network using gradient descent on a loss function. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1cJjCcgc3jgdrsd2nq0AsDq628nK0eUZA\" alt=\"drawing\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "We will use the cross-entropy loss function, which is commonly used for classification tasks (here the network has to classify the stimuli into two categories). The cross-entropy loss function is defined as:\n",
    "\n",
    "$$L = - \\sum_{i=1}^{N} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "where L is the loss, $N$ is the number of samples, $y_i$ is the target output and $\\hat{y}_i$ is the network's prediction for the $i$-th sample.\n",
    "\n",
    "We will also define the optimizer that will be used to train the network. We will use Adam, which is a popular optimizer commonly used in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Define loss: instance of the CrossEntropyLoss class\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=training_kwargs['lr'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Save config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "training_kwargs['env_kwargs'] = env_kwargs\n",
    "training_kwargs['net_kwargs'] = net_kwargs\n",
    "\n",
    "# Save config\n",
    "with open(get_modelpath(envid) / 'config.json', 'w') as f:\n",
    "    json.dump(training_kwargs, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IP_bEakSRgm2"
   },
   "source": [
    "## 2. Supervised training of the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KoNxNIAyRgm3",
    "outputId": "0e1fdba0-6c9f-4c62-da1a-01c31af693a7"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print('Training task ', envid)\n",
    "\n",
    "num_epochs = training_kwargs['n_epochs']\n",
    "\n",
    "# We'll keep track of the loss as we train. \n",
    "# It is initialized to zero and then monitored over training interations\n",
    "running_loss = 0.0 \n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # get inputs and labels and pass them to the GPU\n",
    "    inputs, labels = dataset()\n",
    "    inputs = torch.from_numpy(inputs).type(torch.float).to(device)\n",
    "    labels = torch.from_numpy(labels.flatten()).type(torch.long).to(device)\n",
    "    # print shapes of inputs and labels\n",
    "    if i == 0:\n",
    "        print('inputs shape: ', inputs.shape)\n",
    "        print('labels shape: ', labels.shape)\n",
    "        print('Max labels: ', labels.max())\n",
    "    # we need zero the parameter gradients to re-initialize and avoid they accumulate across epochs\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # INSTRUCTION 3: FORWARD PASS: get the output of the network for a given input\n",
    "    outputs, _ = net(inputs)\n",
    "\n",
    "    #reshape outputs so they have the same shape as labels\n",
    "    outputs = outputs.view(-1, env.action_space.n)\n",
    "\n",
    "    #  INSTRUCTION 4: compute loss with respect to the labels\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # INSTRUCTION 5: compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # INSTRUCTION 6: update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # print average loss over last 200 training iterations and save the current network\n",
    "    running_loss += loss.item()\n",
    "    if i % 200 == 199:\n",
    "        print('{:d} loss: {:0.5f}'.format(i + 1, running_loss / 200))\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # save current state of network's parameters\n",
    "        torch.save(net.state_dict(), get_modelpath(envid) / 'net.pth')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RweGGCYQRgm4"
   },
   "source": [
    "## 3. Run the trained network\n",
    "### (and save the behavioral data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run the network that we have trained. Importantly, we will record the activity of the network and the task variables for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# load configuration file - we might have run the training on the cloud and might now open the results locally\n",
    "with open(get_modelpath(envid) / 'config.json') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will reconstruct the environment from our config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Environment\n",
    "env = gym.make(envid, **config['env_kwargs'])\n",
    "try:\n",
    "    env.timing = config['env_kwargs']['timing']\n",
    "except KeyError:\n",
    "    timing = {}\n",
    "    for period in env.timing.keys():\n",
    "        period_times = [env.sample_time(period) for _ in range(100)]\n",
    "        timing[period] = np.median(period_times)\n",
    "    env.timing = timing\n",
    "env.reset(no_step=True) # this is to initialize the environment "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll make a new instance of the `Net` class and populate it with hyperparameters from our config file, as well as the trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53ea3YbIRgm4",
    "outputId": "01564780-d083-4e81-cbd1-41a15661b809"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Since we will not train the network anymore, we can turn off the gradient computation. The most commun way to do this is to use the context manager torch.no_grad() as follows:\n",
    "with torch.no_grad():\n",
    "    net = Net(input_size=config['net_kwargs']['input_size'],\n",
    "              hidden_size=config['net_kwargs']['hidden_size'],\n",
    "              output_size=config['net_kwargs']['action_size'])\n",
    "    \n",
    "    net = net.to(device) # pass to GPU for running forwards steps\n",
    "    \n",
    "    # load the trained network's weights from the saved file\n",
    "    net.load_state_dict(torch.load(get_modelpath(envid) / 'net.pth'))\n",
    "\n",
    "    # how many trials to run\n",
    "    num_trial = 1000\n",
    "    \n",
    "    # empty lists / dataframe to store activity, choices, and trial information\n",
    "    activity = list()\n",
    "    obs = list()\n",
    "    info = pd.DataFrame()\n",
    "    \n",
    "    for i in range(num_trial):\n",
    "\n",
    "        # create new trial\n",
    "        env.new_trial()\n",
    "        \n",
    "        # read out the inputs in that trial\n",
    "        inputs = torch.from_numpy(env.ob[:, np.newaxis, :]).type(torch.float)\n",
    "        # as before you can print the shapes of the variables to understand what they are and how to use them\n",
    "        # do this for the rest of the variables as you build the code\n",
    "        if i == 0:\n",
    "            print('Shape of inputs: ' + str(inputs.shape))\n",
    "        # INSTRUCTION 7: get the network's prediction for the current input\n",
    "        action_pred, hidden = net(inputs)\n",
    "        action_pred = action_pred.numpy()\n",
    "        \n",
    "        # INSTRUCTION 8: get the network's choice. \n",
    "        # Take into account the shape of action_pred. Remember that the network makes a prediction for each time step in the trial.\n",
    "        # Which is the prediction we really care about when evaluating the network's performance?\n",
    "        choice = np.argmax(action_pred[-1, 0, :])\n",
    "        \n",
    "        # INSTRUCTION 9: check if the choice is correct\n",
    "        # Again, which is the target we want when evaluating the network's performance?\n",
    "        correct = choice == env.gt[-1]\n",
    "\n",
    "        # Log trial info\n",
    "        trial_info = env.trial\n",
    "        trial_info.update({'correct': correct, 'choice': choice}) # Update trial_info with additional information\n",
    "        info = pd.concat([info, pd.DataFrame([trial_info])], ignore_index=True)\n",
    "        \n",
    "        # Log activity\n",
    "        activity.append(np.array(hidden)[:, 0, :])\n",
    "        \n",
    "        # Log the inputs (or observations) received by the network\n",
    "        obs.append(env.ob)\n",
    "\n",
    "    print('Average performance', np.mean(info['correct']))\n",
    "\n",
    "activity = np.array(activity)\n",
    "obs = np.array(obs)\n",
    "\n",
    "# print stats of the activity: max, min, mean, std\n",
    "print('Activity stats:')\n",
    "print('Max: ' + str(np.max(activity)) + \\\n",
    "      ', Min: ' + str(np.min(activity)) + \\\n",
    "      ', Mean: ' + str(np.mean(activity)) + \\\n",
    "      ', Std: ' + str(np.std(activity)) + \\\n",
    "        ', Shape: ' + str(activity.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Network analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Behavioral analysis\n",
    "\n",
    "First, we will do a simple analysis of the network's behavior. Will the network behave similarly to a human participant? We have seen above that its average fraction of correct is at ~ 0.8-0.9. Let's now calculate the psychometric curve, i.e., performance by evidence levels. Use the `info` dataframe as if it was behavioral data from a participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# print the variables in the info dataframe\n",
    "print('Info dataframe:')\n",
    "print(info.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the psychometric curve. You can use the function you wrote in the first tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# plot the probability of choosing right as a function of the signed coherence and then fit a psychometric curve to the data.\n",
    "mpl.rcParams['font.family'] = ['DejaVu Serif']\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import erf\n",
    "def probit(x, beta, alpha):\n",
    "    \"\"\"\n",
    "    Return probit function with parameters alpha and beta.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float\n",
    "        independent variable.\n",
    "    beta : float\n",
    "        sensitiviy.\n",
    "    alpha : TYPE\n",
    "        bias term.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    probit : float\n",
    "        probit value for the given x, beta and alpha.\n",
    "\n",
    "    \"\"\"\n",
    "    probit = 1/2*(1+erf((beta*x+alpha)/np.sqrt(2)))\n",
    "    return probit\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(3, 3), dpi=150)\n",
    "choice = info['choice'].values\n",
    "# translate choice to 0 and 1\n",
    "choice_01 = np.copy(choice)\n",
    "choice_01 -= 1\n",
    "gt = info['ground_truth'].values\n",
    "coherence = info['coh'].values\n",
    "# get signed coherence\n",
    "signed_coherence = np.copy(coherence)\n",
    "signed_coherence[gt == 0] = -signed_coherence[gt == 0]\n",
    "# INSTRUCTION 10: plot the probability of choosing right as a function of the signed coherence\n",
    "for sc in np.unique(signed_coherence):\n",
    "    prob_right = np.mean(choice_01[signed_coherence == sc])\n",
    "    std_right = np.std(choice_01[signed_coherence == sc])/np.sqrt(np.sum(signed_coherence == sc))\n",
    "    ax.errorbar(sc, prob_right, yerr=std_right, color='k')\n",
    "    ax.plot(sc, prob_right, 'o', color='k')\n",
    "    ax.set_xlabel('Signed coherence')\n",
    "    ax.set_ylabel('P(right)')\n",
    "# fit psychometric curve\n",
    "pars, _ = curve_fit(probit, signed_coherence, choice_01, p0=[0, 1])\n",
    "x = np.linspace(-50, 50, 100)\n",
    "ax.plot(x, probit(x, *pars), color='k')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_9lkm4_cRgm5"
   },
   "source": [
    "#### 4.2. neural analysis\n",
    "\n",
    "We will now inspect the network actity and observations. `activity` is of shape (100, 24, 64) (`[ntrials x ntime x nneurons]`), and observations of shape (100, 24, 3) (`[ntrials x ntime x nobs]`). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot an example trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def plot_activity(activity, obs, config, trial):\n",
    "\n",
    "    # Load and preprocess results\n",
    "    f, ax = plt.subplots(figsize=(5, 4), nrows=2, dpi=150)\n",
    "\n",
    "    # time in ms\n",
    "    t_plot = np.arange(activity.shape[1]) * config['dt']\n",
    "\n",
    "    # plot the observations for one trial. Note that we will visualize the inputs as a matrix instead of traces, as we have done before.\n",
    "    im = ax[0].imshow(obs[trial].T, aspect='auto', vmin=0, vmax=1)\n",
    "    ax[0].set_title('Observations')\n",
    "    ax[0].set_ylabel('Stimuli')\n",
    "\n",
    "    # change the xticks to show time in ms\n",
    "    ax[0].set_xticks(np.arange(0, activity.shape[1], 10))\n",
    "    ax[0].set_xticklabels(t_plot[::10])\n",
    "    plt.colorbar(im, ax=ax[0])\n",
    "    # INSTRUCTION 11: plot the activity for one trial\n",
    "    im = ax[1].imshow(activity[trial].T, aspect='auto', cmap='viridis')\n",
    "    ax[1].set_title('Activity')\n",
    "    ax[1].set_xlabel('Time (ms)')\n",
    "    ax[1].set_ylabel('Neurons')\n",
    "    plt.colorbar(im, ax=ax[1])\n",
    "    # change the xticks to show time in ms\n",
    "    ax[1].set_xticks(np.arange(0, activity.shape[1], 10))\n",
    "    ax[1].set_xticklabels(t_plot[::10])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_activity(activity=activity, obs=obs, config=config, trial=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that many neurons are completely silent. Is this always like this? Let's identify and exclude the silent neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "silent_idx = np.where(activity.sum(axis=(0, 1))==0)[0]\n",
    "\n",
    "print('fraction of silent neurons:', len(silent_idx)/activity.shape[-1])\n",
    "# INSRTUCTION 12: plot the activity for one trial, but now excluding the silent neurons\n",
    "clean_activity = activity[:,:,np.delete(np.arange(activity.shape[-1]), silent_idx)]\n",
    "plot_activity(activity=clean_activity, obs=obs, config=config, trial=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You see that 50% of the neurons are silent. Why do you think this happens?\n",
    " \n",
    " Also, as in biological networks, the active units of the RNN have different levels of activity. We will thus normalize the acitvity across all trials (so that all neurons have the same minimum and maximum activity) and plot again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# min_max scaling\n",
    "minmax_activity = np.array([neuron-neuron.min() for neuron in clean_activity.transpose(2,0,1)]).transpose(1,2,0)\n",
    "minmax_activity = np.array([neuron/neuron.max() for neuron in minmax_activity.transpose(2,0,1)]).transpose(1,2,0)\n",
    "\n",
    "plot_activity(activity=minmax_activity, obs=obs, config=config, trial=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that neurons have mixed activtity profiles, corresponding to the different task periods. Can you figure out the role played by some of these neurons?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Peristimulus histograms for the different choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nGqFPXUoRgm6",
    "outputId": "0892bce5-e128-4ea0-fa54-cfb4066eb8f0"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def analysis_activity_by_condition(activity, info, config, conditions=['choice']):\n",
    "    \"\"\"\n",
    "    Plot single neuron activity by condition.\n",
    "    \"\"\"   \n",
    "    for condition in conditions:\n",
    "        values = pd.unique(info[condition])\n",
    "        f, ax = plt.subplots(figsize=(10, 3), ncols=len(values), sharex=True, dpi=150)\n",
    "        t_plot = np.arange(activity.shape[1]) * config['dt']\n",
    "        for i_v, value in enumerate(values):\n",
    "            # INSTRUCTION 13: plot the average activity across neurons and trials for each condition\n",
    "            a = activity[info[condition] == value]\n",
    "            ax[i_v].imshow(a.mean(axis=0).T, aspect='auto', cmap='viridis')\n",
    "            ax[i_v].set_xlabel('Time (ms)')\n",
    "            ax[i_v].set_ylabel('Mean activity for ' + condition + ' = ' + str(value))\n",
    "            # change the xticks to show time in ms\n",
    "            ax[1].set_xticks(np.arange(0, activity.shape[1], 10))\n",
    "            ax[1].set_xticklabels(t_plot[::10])\n",
    "\n",
    "        # plt.legend(title=condition, loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "\n",
    "analysis_activity_by_condition(minmax_activity, info, config, conditions=['choice']) # other conditions: correct, ground_truth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are neurons that are more active for one choice than for the other. And others are active for both, what do you think they are doing?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Stimulus and choice decoding from hidden unit activity\n",
    "\n",
    "We will now look at the information content in the hidden layers: How long does it take (after stimulus presentation) until the RNN represents stimuli reliably? We are going to use linear discriminant analysis (`sklda.LinearDiscriminantAnalysis`), which is a simple linear classifier. \n",
    "\n",
    "For each time bin, we will divide the `ntrials x nneurons` activity matrix, as well as the `ntrials` vector of labels (`info.ground_truth.values`) into a train and test set by calling `sklms.train_test_split()`. We will then fit LDA weights on the train set, and predict labels (left vs. right) from the test set activity to calculate the classification accuracy. \n",
    "\n",
    "Repeat that procedure for 100 train-test-splits to calculate the mean classification accuracy for each timepoint, as well as 95% confidence intervals over train-test-splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection as sklms\n",
    "import sklearn.discriminant_analysis as sklda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# number of CV splits\n",
    "n_splits = 100\n",
    "\n",
    "# set\n",
    "mean_acc = np.zeros([n_splits, minmax_activity.shape[1]]) * np.nan\n",
    "\n",
    "for i in range(n_splits):    \n",
    "    \n",
    "    # transpose tensor to be shape [trials, time, neurons]\n",
    "    for xi,x in enumerate(minmax_activity.transpose(1,0,2)):\n",
    "\n",
    "        # INSTRUCTION 14: split data into train and test sets using sklms.\n",
    "        x_train, x_test, y_train, y_test = sklms.train_test_split(x, info.ground_truth.values, random_state=i)\n",
    "\n",
    "        # INSTRUCTION 15: fit a linear discriminant analysis model to the training data using sklda\n",
    "        lda_fitted = sklda.LinearDiscriminantAnalysis(solver='lsqr').fit(X=x_train, y=y_train)\n",
    "\n",
    "        # INSTRUCTION 16: predict the labels for the test data\n",
    "        y_pred = lda_fitted.predict(x_test)\n",
    "\n",
    "        # INSTRUCTION 17: compute the accuracy of the model\n",
    "        correct = 1 - np.abs(y_pred - y_test)\n",
    "\n",
    "        mean_acc[i,xi] = correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# calculate 95% CI\n",
    "ci_acc = np.percentile(mean_acc, [5,95], axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the classification accuracy (together with the 95% CI) over the course of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# for plotting: time axis, stim and resp times\n",
    "t_plot = np.arange(activity.shape[1]) * config['dt']\n",
    "stim_onset = t_plot[np.where(obs[0,:,1]!=0)[0][0]]\n",
    "resp_onset= t_plot[np.where(obs[0,:,0]!=1)[0][0]]\n",
    "\n",
    "# plot linear classification accuracy\n",
    "plt.figure(figsize=(4,3), dpi=150)\n",
    "plt.plot(t_plot, np.zeros(mean_acc.shape[1])+.5, 'k--', alpha=.2)\n",
    "plt.plot(stim_onset, .48, '^', color = 'r', ms=10)\n",
    "plt.plot(resp_onset, .48, '^', color='b', ms=10)\n",
    "plt.plot(t_plot, np.mean(mean_acc, axis=0), 'k')\n",
    "plt.fill_between(t_plot, ci_acc[0], ci_acc[1], color='k', alpha=.2)\n",
    "plt.ylabel('classification accuracy')\n",
    "plt.xlabel('time')\n",
    "plt.xlim(t_plot[0],t_plot[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that before stimulus presentation, classification accuracy is at chance. In contrast, information about the stimulus rises quickly after. \n",
    "\n",
    "Will these network dynamics look different for stimuli of different coherence levels? Repeat the same analysis for each level of coherence (`info.coh.values`) and plot the mean accuracy with the pre-specified color gradient. To keep the plot clean, skip confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "mean_acc = np.zeros([len(np.unique(info.coh.values)), n_splits, minmax_activity.shape[1]])\n",
    "\n",
    "for ci,c in enumerate(np.unique(info.coh.values)):\n",
    "    \n",
    "    # INSTRUCTION 18: get the indices of the trials with the current coherence\n",
    "    cidx = np.where(info.coh.values==c)\n",
    "    \n",
    "    for i in range(n_splits):    \n",
    "\n",
    "        # transpose tensor to be shape [trials, time, neurons]\n",
    "        for xi,x in enumerate(minmax_activity[cidx].transpose(1,0,2)):\n",
    "\n",
    "            # train-test-split\n",
    "            x_train, x_test, y_train, y_test = sklms.train_test_split(x, info.ground_truth.values[cidx], random_state=i)\n",
    "\n",
    "            # fit to train data\n",
    "            lda_fitted = sklda.LinearDiscriminantAnalysis(solver='lsqr').fit(X=x_train, y=y_train)\n",
    "\n",
    "            # predict test set labels\n",
    "            y_pred = lda_fitted.predict(x_test)\n",
    "\n",
    "            # is the response correct for each trial?\n",
    "            correct = 1 - np.abs(y_pred - y_test)\n",
    "\n",
    "            mean_acc[ci,i,xi] = correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'unity' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n unity ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# colors corresponding to different values of color gradient\n",
    "colors = plt.get_cmap('magma')(np.linspace(0.1,.9, len(np.unique(info.coh.values))))\n",
    "\n",
    "# plot linear classification accuracy\n",
    "plt.figure(figsize=(4,3), dpi=120)\n",
    "\n",
    "# plot mean acc for each coherence level\n",
    "for ci in range(len(np.unique(info.coh.values))):\n",
    "    plt.plot(t_plot, np.mean(mean_acc[ci], axis=0), color=colors[ci], label=np.unique(info.coh.values)[ci])\n",
    "\n",
    "plt.plot(t_plot, np.zeros(mean_acc[ci].shape[1])+.5, 'k--', alpha=.2)    \n",
    "plt.plot(stim_onset, .48, '^', color = 'k', ms=10)\n",
    "plt.plot(resp_onset, .48, '^', color='k', ms=10)\n",
    "plt.ylabel('classification accuracy')\n",
    "plt.xlabel('time')\n",
    "plt.xlim(t_plot[0], t_plot[-1])\n",
    "plt.legend(frameon=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
