{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Initialize weights and biases for Linear layer\n",
    "        self.W = nn.Parameter(torch.randn(output_size, input_size) * 0.01)\n",
    "        self.b = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the linear layer\n",
    "        x = torch.matmul(x, self.W.T) + self.b\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Initialize weights and biases for the first Linear layer\n",
    "        self.W_ih = nn.Parameter(torch.randn(hidden_size, input_size) * 0.01)\n",
    "        self.b_ih = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        # Initialize weights and biases for the second Linear layer\n",
    "        self.W_ho = nn.Parameter(torch.randn(output_size, hidden_size) * 0.01)\n",
    "        self.b_ho = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the first linear layer with ReLU activation\n",
    "        h = torch.relu(torch.matmul(x, self.W_ih.T) + self.b_ih)\n",
    "\n",
    "        # Apply the second linear layer\n",
    "        x = torch.matmul(h, self.W_ho.T) + self.b_ho\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Initialize weights and biases for RNN\n",
    "        self.W_ih = nn.Parameter(torch.randn(hidden_size, input_size) * 0.01)\n",
    "        self.W_hh = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.01)\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        # Initialize weights and biases for Linear layer\n",
    "        self.W = nn.Parameter(torch.randn(output_size, hidden_size) * 0.01)\n",
    "        self.b = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "        h = torch.zeros(batch_size, self.hidden_size, device=x.device)  # Initial hidden state\n",
    "\n",
    "        # Apply the RNN step by step\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[t, :, :]\n",
    "            h = nn.functional.relu(torch.matmul(x_t, self.W_ih.T) + torch.matmul(h, self.W_hh.T) + self.b_h)\n",
    "            outputs.append(h.unsqueeze(0))\n",
    "\n",
    "        out = torch.cat(outputs, dim=0)\n",
    "\n",
    "        # Apply the linear layer\n",
    "        x = torch.matmul(out, self.W.T) + self.b\n",
    "        return x, out"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
