{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks: Perceptron and Multi-Layer Perceptron\n",
    "\n",
    "In this tutorial, we will learn how to use Perceptron and Multi-Layer Perceptron (MLP) neural networks to solve simple classification tasks. We will walk through the necessary steps to implement and train these networks using PyTorch.\n",
    "\n",
    "## Index\n",
    "1. Imports\n",
    "2. Models\n",
    "    - Perceptron\n",
    "    - Multi-Layer Perceptron\n",
    "2. Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "The Perceptron is one of the simplest types of artificial neural networks. It consists of a single layer of neurons, each with a set of weights and biases. The output of the Perceptron is calculated as the weighted sum of the inputs plus the bias, passed through an activation function.\n",
    "\n",
    "$$z = \\mathbf{w} \\cdot \\mathbf{x} + b = w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n + b$$\n",
    "\n",
    "$$y = \\gamma(z)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbf{w}$ is the weight vector\n",
    "- $\\mathbf{x}$ is the input vector\n",
    "- $b$ is the bias\n",
    "- $\\gamma$ is the sigmoid function.\n",
    "\n",
    "To implement the perceptron in PyTorch, we can create a class that inherits from `torch.nn.Module` and the define the different elements of the network in the constructor and the forward pass in the `forward` method. You can use the matrix multiplication operation `torch.matmul` to calculate the weighted sum of the inputs and the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(output_size, input_size) * 0.01)\n",
    "        self.b = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.matmul(x, self.W.T) + self.b\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron\n",
    "\n",
    "THe multi-layer perceptron (MLP) is a type of feedforward neural network that consists of multiple layers of neurons. Each layer is fully connected to the next layer. The formula to calculate the output of an MLP is similar to the perceptron, but with the addition of non-linear activation functions between the layers:\n",
    "\n",
    "$$z^{(1)} = \\mathbf{W}^{(1)} \\cdot \\mathbf{x} + \\mathbf{b}^{(1)}$$\n",
    "$$h^{(1)} = \\phi(z^{(1)})$$\n",
    "$$z^{(2)} = \\mathbf{W}^{(2)} \\cdot \\mathbf{h}^{(1)} + \\mathbf{b}^{(2)}$$\n",
    "$$y = \\gamma(z^{(2)})$$\n",
    "\n",
    "Where:\n",
    "- x is the input vector.\n",
    "- $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$ are the weight matrices of the first and second layers.\n",
    "- $\\mathbf{b}^{(1)}$ and $\\mathbf{b}^{(2)}$ are the bias vectors of the first and second layers.\n",
    "- $\\phi$ is the activation function. In this case, we will use the ReLU activation function.\n",
    "- $\\gamma$ is the output activation function. In this case, we will use the sigmoid activation function.\n",
    "\n",
    "\n",
    "Note that if we don't include any non-linear activation functions, the MLP is equivalent to a linear regression model. However, by adding non-linear activation functions, the MLP can learn complex patterns in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Initialize weights and biases for the first Linear layer\n",
    "        self.W_ih = nn.Parameter(torch.randn(hidden_size, input_size) * 0.01)\n",
    "        self.b_ih = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        # Initialize weights and biases for the second Linear layer\n",
    "        self.W_ho = nn.Parameter(torch.randn(output_size, hidden_size) * 0.01)\n",
    "        self.b_ho = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "    def forward(self, x, non_linear_activation='relu'):\n",
    "        if non_linear_activation=='relu':\n",
    "            # Apply the first linear layer with ReLU activation\n",
    "            h = torch.relu(torch.matmul(x, self.W_ih.T) + self.b_ih)\n",
    "        elif non_linear_activation=='sigmoid':\n",
    "            # Apply the first linear layer with Sigmoid activation\n",
    "            h = torch.sigmoid(torch.matmul(x, self.W_ih.T) + self.b_ih)\n",
    "        else:\n",
    "            h = torch.matmul(x, self.W_ih.T) + self.b_ih\n",
    "\n",
    "        # Apply the second linear layer\n",
    "        x = torch.matmul(h, self.W_ho.T) + self.b_ho\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, Y, title, model=None):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))\n",
    "    grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "    if model is not None:\n",
    "        with torch.no_grad():\n",
    "            Z = model(grid)\n",
    "            Z = torch.sigmoid(Z).numpy()  # Apply sigmoid to the output\n",
    "            Z = (Z > 0.5).astype(int)\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', marker='o')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def generate_linear_data(N=100, D=2):\n",
    "    X = np.random.randn(N, D)\n",
    "    X[:N//2, :] += 1\n",
    "    X[N//2:, :] -= 1\n",
    "    Y = np.concatenate((np.zeros(N//2), np.ones(N//2)))\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    Y = torch.tensor(Y, dtype=torch.long)\n",
    "    return X, Y\n",
    "\n",
    "def generate_xor_data(n_points=100):\n",
    "    # Base XOR points\n",
    "    base_points = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "    base_labels = np.array([0, 1, 1, 0], dtype=np.float32)\n",
    "    \n",
    "    # Generate more points around the base points\n",
    "    X = []\n",
    "    Y = []\n",
    "    for _ in range(n_points // 4):\n",
    "        for point, label in zip(base_points, base_labels):\n",
    "            noise = np.random.normal(0, 0.1, size=point.shape)\n",
    "            X.append(point + noise)\n",
    "            Y.append(label)\n",
    "    \n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    Y = np.array(Y, dtype=np.float32)\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "# Task 1: Linearly Separate Two Clouds of Dots\n",
    "X_linear, Y_linear = generate_linear_data()\n",
    "input_size = 2\n",
    "output_size = 1\n",
    "plot_decision_boundary(X_linear, Y_linear, 'Linear Data')\n",
    "X_xor, Y_xor = generate_xor_data()\n",
    "plot_decision_boundary(X_xor, Y_xor, 'XOR Data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,  X, Y, num_epochs=1000, print_interval=100):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, Y.float().view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % print_interval == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Linearly Separate Two Clouds of Dots\n",
    "X_linear, Y_linear = generate_linear_data()\n",
    "input_size = 2\n",
    "output_size = 1\n",
    "\n",
    "# Perceptron\n",
    "model_perceptron = Perceptron(input_size, output_size)\n",
    "train_model(model_perceptron, X_linear, Y_linear, num_epochs=1000, print_interval=100)\n",
    "plot_decision_boundary(X_linear, Y_linear, 'Perceptron - Linear Data', model=model_perceptron)\n",
    "\n",
    "# MLP\n",
    "hidden_size = 2\n",
    "model_mlp = MLP(input_size, hidden_size, output_size)\n",
    "train_model(model_mlp, X_linear, Y_linear, num_epochs=1000, print_interval=100)\n",
    "plot_decision_boundary(X_linear, Y_linear, 'MLP - Linear Data', model=model_mlp)\n",
    "\n",
    "# Task 2: XOR Task\n",
    "X_xor, Y_xor = generate_xor_data()\n",
    "\n",
    "# Perceptron\n",
    "model_perceptron = Perceptron(input_size, output_size)\n",
    "train_model(model_perceptron, X_xor, Y_xor, num_epochs=10000, print_interval=1000)\n",
    "plot_decision_boundary(X_xor, Y_xor, 'Perceptron - XOR Data', model=model_perceptron)\n",
    "\n",
    "# MLP\n",
    "model_mlp = MLP(input_size, hidden_size, output_size)\n",
    "train_model(model_mlp, X_xor, Y_xor, num_epochs=10000, print_interval=1000)\n",
    "plot_decision_boundary(X_xor, Y_xor, 'MLP - XOR Data', model=model_mlp)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
